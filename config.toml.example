# Example configuration for lite-chat (new provider schema)
# Copy this file to ~/.lite-chat/config.toml and customize as needed

[general]

# Log file path (optional, defaults to stderr if not specified)
# Use an absolute path or ~ for home directory
log_level = "INFO"  # Options: DEBUG, INFO, WARN, ERROR, CRITICAL

# Log file path (optional, defaults to stderr if not specified)
# Use an absolute path or ~ for home directory
# log_file = "~/.lite-chat/logs/litechat.log"

# Directory for storing conversations
conversations_dir = "~/.lite-chat/conversations"

# Directory for exports (optional). If not set, exports go to the current working directory.
# exports_dir = "~/.lite-chat/exports"

# Streaming (optional; default false)
# enable_streaming = true

# Default provider/model
default_provider = "ollama"
default_model = "llama3.2"

# System prompt to use for all conversations
system_prompt = "You are a helpful assistant."

# Default temperature for new conversations (0.0 - 2.0)
default_temperature = 0.7

# API request timeout in seconds (default: 1200 = 20 minutes)
# Increase this if you get timeout errors with large models or long conversations
timeout = 1200

# Provider definitions. Add as many as you need.

# Ollama provider with contexts
[[providers]]
id = "ollama"
type = "ollama"
api_url = "http://localhost:11434/api"
models = [
  { name = "llama3.2", contexts = "8192" },
  { name = "phi3",     contexts = "8192,32768" }
]

# OpenAI-compatible provider (OpenAI)
[[providers]]
id = "openai"
type = "openai-compatible"
api_url = "https://api.openai.com"
api_key = "sk-..."
models = "gpt-5-mini"
# headers = { OpenAI-Organization = "org_id" }

# OpenAI-compatible provider (DeepSeek)
[[providers]]
id = "deepseek"
type = "openai-compatible"
api_url = "https://api.deepseek.com"
api_key = "sk-..."
models = "deepseek-chat,deepseek-reasoner"
