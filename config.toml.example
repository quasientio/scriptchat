# Example configuration for ScriptChat (new provider schema)
# Copy this file to ~/.scriptchat/config.toml and customize as needed

[general]

# Log file path (optional, defaults to stderr if not specified)
# Use an absolute path or ~ for home directory
log_level = "INFO"  # Options: DEBUG, INFO, WARN, ERROR, CRITICAL

# Log file path (optional, defaults to stderr if not specified)
# Use an absolute path or ~ for home directory
# log_file = "~/.scriptchat/logs/scriptchat.log"

# Directory for storing conversations
conversations_dir = "~/.scriptchat/conversations"

# Directory for exports (optional). If not set, exports go to the current working directory.
# exports_dir = "~/.scriptchat/exports"

# Streaming (optional; default false)
# enable_streaming = true

# Default model in provider/model format (e.g., "ollama/llama3.2", "openai/gpt-4o")
default_model = "ollama/llama3.2"

# System prompt to use for all conversations
system_prompt = "You are a helpful assistant."

# Default temperature for new conversations (0.0 - 2.0)
default_temperature = 0.7

# API request timeout in seconds (default: 1200 = 20 minutes)
# Increase this if you get timeout errors with large models or long conversations
timeout = 1200

# File registration confirmation threshold in bytes. Above this size, use /file --force.
file_confirm_threshold_bytes = 40000

# Environment variable expansion in scripts
# When true, ${VAR} falls back to environment variables if not set via /set
# Sensitive patterns (*_KEY, *_SECRET, *_TOKEN, *_PASSWORD) are blocked by default
env_expand_from_environment = true

# Override the default blocklist with custom patterns (not merged, replaces defaults)
# Uses wildcard matching (e.g., "*_PRIVATE" matches "MY_PRIVATE", "APP_PRIVATE")
# env_var_blocklist = ["MY_PRIVATE_*", "INTERNAL_*"]
# Use empty list to allow all env vars: env_var_blocklist = []

# Thinking/reasoning model settings
# For models like Kimi, DeepSeek R1, Claude with extended thinking, etc.
# Thinking content is displayed in the UI but NOT sent back in conversation history by default
# Set to true to include thinking content when sending messages to the API
# include_thinking_in_history = false

# Provider definitions. Add as many as you need.
#
# API keys can be set in config or via environment variables.
# If api_key is empty/missing, ScriptChat will look for {PROVIDER_ID}_API_KEY
# (e.g., OPENAI_API_KEY, DEEPSEEK_API_KEY, ANTHROPIC_API_KEY).

# Ollama provider with context window sizes
[[providers]]
id = "ollama"
type = "ollama"
api_url = "http://localhost:11434/api"
models = [
  { name = "llama3.2", context = 8192 },
  { name = "phi3",     context = 32768 }
]

# OpenAI-compatible provider (OpenAI)
# Uses Responses API by default (store=false) when id="openai"
[[providers]]
id = "openai"
type = "openai-compatible"
api_url = "https://api.openai.com"
# api_key = "sk-..."  # Or set OPENAI_API_KEY env var
# api_format = "responses"  # Default for id="openai"; use "chat" for Chat Completions API
models = [
  { name = "gpt-5-mini", reasoning_levels = ["minimal", "medium", "high"], reasoning_default = "medium" },
  { name = "gpt-5.1-mini", reasoning_levels = ["none", "low", "medium", "high"], reasoning_default = "none" }
]
# headers = { OpenAI-Organization = "org_id" }

# OpenAI-compatible provider (DeepSeek direct - no zero retention policy)
[[providers]]
id = "deepseek"
type = "openai-compatible"
api_url = "https://api.deepseek.com"
# api_key = "sk-..."  # Or set DEEPSEEK_API_KEY env var
models = "deepseek-chat,deepseek-reasoner"

# Fireworks AI - zero data retention by default
# Use prompt_cache = false to also disable volatile memory caching
[[providers]]
id = "fireworks"
type = "openai-compatible"
api_url = "https://api.fireworks.ai/inference"
# api_format = "responses"  # Optional: use Responses API with store=false (only some models)
prompt_cache = false  # Disable prompt caching for full privacy (no data in volatile memory)
# api_key = "..."  # Or set FIREWORKS_API_KEY env var
# Models can have optional aliases for shorter /model commands (e.g., /model dsv3)
# Aliases must be unique across all providers and contain only alphanumeric, underscore, dash, or dot.
# Thinking models need high max_tokens to allow room for reasoning + output.
# Streaming is auto-enabled when max_tokens > 4096 (required by some providers).
# Use skip_prompt_cache_param = true for models that don't support prompt_cache_max_len parameter.
# Use reasoning_levels for models that support /reason command (Fireworks uses reasoning_effort param).
models = [
  { name = "accounts/fireworks/models/gpt-oss-120b", alias = "gpt-oss", reasoning_levels = ["low", "medium", "high"] },
  { name = "accounts/fireworks/models/deepseek-v3", alias = "dsv3" },
  { name = "accounts/fireworks/models/deepseek-r1", alias = "dsr1" },
  { name = "accounts/fireworks/models/kimi-k2-thinking", alias = "kimi", max_tokens = 16384, skip_prompt_cache_param = true, reasoning_levels = ["low", "medium", "high"] }
]

# Baseten - serverless inference
[[providers]]
id = "baseten"
type = "openai-compatible"
api_url = "https://inference.baseten.co/v1"
auth_format = "api-key"  # Baseten uses "Api-Key" auth header instead of "Bearer"
# api_key = "..."  # Or set BASETEN_API_KEY env var
models = [
  { name = "Qwen/Qwen3-Coder-480B-A35B-Instruct" }
]

# Anthropic Claude provider
[[providers]]
id = "anthropic"
type = "anthropic"
api_url = "https://api.anthropic.com"
# api_key = "sk-ant-..."  # Or set ANTHROPIC_API_KEY env var
models = [
  { name = "claude-sonnet-4-20250514", reasoning_levels = ["low", "medium", "high", "max"] },
  { name = "claude-3-5-sonnet-20241022" },
  { name = "claude-3-5-haiku-20241022" }
]
