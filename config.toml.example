# Example configuration for ScriptChat (new provider schema)
# Copy this file to ~/.scriptchat/config.toml and customize as needed

[general]

# Log file path (optional, defaults to stderr if not specified)
# Use an absolute path or ~ for home directory
log_level = "INFO"  # Options: DEBUG, INFO, WARN, ERROR, CRITICAL

# Log file path (optional, defaults to stderr if not specified)
# Use an absolute path or ~ for home directory
# log_file = "~/.scriptchat/logs/scriptchat.log"

# Directory for storing conversations
conversations_dir = "~/.scriptchat/conversations"

# Directory for exports (optional). If not set, exports go to the current working directory.
# exports_dir = "~/.scriptchat/exports"

# Streaming (optional; default false)
# enable_streaming = true

# Default provider/model
default_provider = "ollama"
default_model = "llama3.2"

# System prompt to use for all conversations
system_prompt = "You are a helpful assistant."

# Default temperature for new conversations (0.0 - 2.0)
default_temperature = 0.7

# API request timeout in seconds (default: 1200 = 20 minutes)
# Increase this if you get timeout errors with large models or long conversations
timeout = 1200

# File registration confirmation threshold in bytes. Above this size, use /file --force.
file_confirm_threshold_bytes = 40000

# Environment variable expansion in scripts
# When true, ${VAR} falls back to environment variables if not set via /set
# Sensitive patterns (*_KEY, *_SECRET, *_TOKEN, *_PASSWORD) are blocked by default
env_expand_from_environment = true

# Override the default blocklist with custom patterns (not merged, replaces defaults)
# Uses wildcard matching (e.g., "*_PRIVATE" matches "MY_PRIVATE", "APP_PRIVATE")
# env_var_blocklist = ["MY_PRIVATE_*", "INTERNAL_*"]
# Use empty list to allow all env vars: env_var_blocklist = []

# Provider definitions. Add as many as you need.
#
# API keys can be set in config or via environment variables.
# If api_key is empty/missing, ScriptChat will look for {PROVIDER_ID}_API_KEY
# (e.g., OPENAI_API_KEY, DEEPSEEK_API_KEY, ANTHROPIC_API_KEY).

# Ollama provider with contexts
[[providers]]
id = "ollama"
type = "ollama"
api_url = "http://localhost:11434/api"
models = [
  { name = "llama3.2", contexts = "8192" },
  { name = "phi3",     contexts = "8192,32768" }
]

# OpenAI-compatible provider (OpenAI)
# Uses Responses API by default (store=false) when id="openai"
[[providers]]
id = "openai"
type = "openai-compatible"
api_url = "https://api.openai.com"
# api_key = "sk-..."  # Or set OPENAI_API_KEY env var
# api_format = "responses"  # Default for id="openai"; use "chat" for Chat Completions API
models = [
  { name = "gpt-5-mini", reasoning_levels = ["minimal", "medium", "high"], reasoning_default = "medium" },
  { name = "gpt-5.1-mini", reasoning_levels = ["none", "low", "medium", "high"], reasoning_default = "none" }
]
# headers = { OpenAI-Organization = "org_id" }

# OpenAI-compatible provider (DeepSeek direct - no zero retention policy)
[[providers]]
id = "deepseek"
type = "openai-compatible"
api_url = "https://api.deepseek.com"
# api_key = "sk-..."  # Or set DEEPSEEK_API_KEY env var
models = "deepseek-chat,deepseek-reasoner"

# Fireworks AI - zero data retention by default
# Use prompt_cache = false to also disable volatile memory caching
[[providers]]
id = "fireworks"
type = "openai-compatible"
api_url = "https://api.fireworks.ai/inference"
# api_format = "responses"  # Optional: use Responses API with store=false (only some models)
prompt_cache = false  # Disable prompt caching for full privacy (no data in volatile memory)
# api_key = "..."  # Or set FIREWORKS_API_KEY env var
models = [
  { name = "accounts/fireworks/models/gpt-oss-120b" },
  { name = "accounts/fireworks/models/deepseek-v3" },
  { name = "accounts/fireworks/models/deepseek-r1" }
]

# Anthropic Claude provider
[[providers]]
id = "anthropic"
type = "anthropic"
api_url = "https://api.anthropic.com"
# api_key = "sk-ant-..."  # Or set ANTHROPIC_API_KEY env var
models = [
  { name = "claude-sonnet-4-20250514", reasoning_levels = ["low", "medium", "high", "max"] },
  { name = "claude-3-5-sonnet-20241022" },
  { name = "claude-3-5-haiku-20241022" }
]
default_model = "claude-sonnet-4-20250514"
